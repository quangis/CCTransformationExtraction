# Name:        Interpreting Geo-analytical Questions as core concept transformations
# Purpose:     Python script for identifying placenames, entities, and core concepts in each geo-analytical question,
#              then extracting core concept transformations from parser trees generated by a Antlr4 grammar-GenAnQu.g4.

import json
# [X] import nlp packages for placename and entities recognition
from spacy.lang.en import English
import en_core_web_sm
from spacy.matcher import PhraseMatcher
import nltk
import nltk.tokenize as nt
from allennlp.predictors.predictor import Predictor  # For using ELMo-based NER & Fine Grained NER
from word2number import w2n
# [X] import antlr4 grammar
from antlr4 import *
from Grammar.GeoAnQuLexer import GeoAnQuLexer
from Grammar.GeoAnQuParser import GeoAnQuParser
from antlr4.tree.Trees import Trees
from antlr4.error.ErrorListener import ErrorListener


# [X]  customized a list of stopwords
class CustomEnglishDefaults(English.Defaults):
    # stop_words = set(["is", "are", "was", "were", "do", "does", "did", "have", "had", "the"])
    stop_words = {"do", "did", "does", "a", "an", "the", "their", 'his', 'her', 'my'}


class CustomEnglish(English):
    lang = "custom_en"
    Defaults = CustomEnglishDefaults


# [X] Raise exception for errors in parsing questions, such as token recognition error.
class MyErrorListener(ErrorListener):
    def __init__(self):
        super(MyErrorListener, self).__init__()

    def syntaxError(self, recognizer, offendingSymbol, line, column, msg, e):
        raise Exception()

    def reportAmbiguity(self, recognizer, dfa, startIndex, stopIndex, exact, ambigAlts, configs):
        raise Exception()

    def reportAttemptingFullContext(self, recognizer, dfa, startIndex, stopIndex, conflictingAlts, configs):
        raise Exception()

    def reportContextSensitivity(self, recognizer, dfa, startIndex, stopIndex, prediction, configs):
        raise Exception()


class BracketMatch:
    def __init__(self, refstr, parent=None, start=-1, end=-1):
        self.parent = parent
        self.start = start
        self.end = end
        self.refstr = refstr
        self.nested_matches = []

    def __str__(self):
        cur_index = self.start + 1
        result = ""
        if self.start == -1 or self.end == -1:
            return ""
        for child_match in self.nested_matches:
            if child_match.start != -1 and child_match.end != -1:
                result += self.refstr[cur_index:child_match.start]
                cur_index = child_match.end + 1
            else:
                continue
        result += self.refstr[cur_index:self.end]
        return result


def is_left_inside(string, list):
    cur_list = []
    for l in list:
        if l.lower().strip().startswith(string):
            cur_list.append(l)
    return cur_list


# [X] Convert numeric words into digit numbers
# Input sentence(string): 'What is average network distance for three thousand and five people to
# two hundred and twelve closest primary schools'.
# Output sentence(string): 'What is average network distance for 3005 people to 212 closest primary schools'.
# except for 'five star hotels'
def word2num(sentence):
    try:
        if 'five star' not in sentence:
            cur_doc = nlp(sentence)
            numWords = ''
            numDig = ''
            for cur_i in range(0, len(cur_doc)):
                if cur_doc[cur_i].pos_ == 'NUM':
                    numWords = numWords + ' ' + cur_doc[cur_i].text
                    cur_i += 1
                elif cur_doc[cur_i].text == 'and' and cur_doc[cur_i - 1].pos_ == 'NUM':
                    numWords = numWords + ' and'
                    cur_i += 1
                elif numWords and not cur_doc[cur_i].pos_ == 'NUM':
                    numDig = w2n.word_to_num(numWords.strip())
                    # print(numWords)
                    # print(numDig)
                    sentence = sentence.replace(numWords.strip(), str(numDig))
                    numWords = ''
    except:
        return sentence

    return sentence


# [X] Identify Place names(e.g., ) in questions
# input string sentence:
# 'What buildings are within 1 minute of driving time from a fire station for
# Multifunctional Urban Area in Fort Worth in US
# output tuple:
# (['Multifunctional Urban Area', 'Fort Worth', 'US'],
# 'What buildings are within 1 minute of driving time from a fire station
# for each PlaceName0 in PlaceName1 in PlaceName3')
def place_ner(sentence):
    # predictorELMo = Predictor.from_path(
    #     "https://storage.googleapis.com/allennlp-public-models/ner-model-2020.02.10.tar.gz")
    pred = predictorELMo.predict(sentence)

    PlaceName = []
    loc = 0
    for i in range(0, len(pred['tags'])):
        if pred['tags'][i] == 'U-LOC' or pred['tags'][i] == 'U-PER':  # place name is a single word, such as Utrecht
            if not pred['words'][i] == 'PC4':
                PlaceName.append(pred['words'][i])
                sentence = sentence.replace(pred['words'][i], 'PlaceName' + str(loc))
                loc += 1
        elif pred['tags'][i] == 'B-LOC':  # When place name is a phrase, such as Happy Valley
            place = pred['words'][i]
        elif pred['tags'][i] == 'I-LOC' or pred['tags'][i] == 'L-LOC':
            place = place + ' ' + pred['words'][i]
            if i + 1 == len(pred['tags']):
                PlaceName.append(place)
                sentence = sentence.replace(place, 'PlaceName' + str(loc))
                place = ''
            elif pred['tags'][i + 1] == 'O':  # 'O' not a place name
                PlaceName.append(place)
                sentence = sentence.replace(place, 'PlaceName' + str(loc))
                loc += 1
                place = ''

    # Solve place name + place type, such as PlaceName0 area(PC4 area) -> PlaceName0(PC4 area)...
    cur_words = sentence.strip().split(' ')
    for i in range(0, len(cur_words)):
        if cur_words[i].startswith('PlaceName'):
            if i + 1 < len(cur_words):
                if cur_words[
                    i + 1] in pt_set:  # PlaceName0(Happy Valley) ski resort -> PlaceName0(Happy Valley ski resort)
                    cur_index = int(cur_words[i][9:])  # PlaceName0 -> 0
                    PlaceName[cur_index] = PlaceName[cur_index] + ' ' + cur_words[i + 1]
                    sentence = sentence.replace(' '.join(cur_words[i:i + 2]), cur_words[i])
                elif not len(is_left_inside(cur_words[i + 1],
                                            pt_set)) == 0:  # PlaceName0 ski resort(Happy Valley ski resort) -> PlaceName0
                    if i + 2 < len(cur_words):
                        cur_pt = cur_words[i + 1] + ' ' + cur_words[i + 2]
                        if cur_pt in is_left_inside(cur_words[i + 1], pt_set):
                            cur_index = int(cur_words[i][9:])  # PlaceName0 -> 0
                            PlaceName[cur_index] = PlaceName[cur_index] + ' ' + cur_pt
                            sentence = sentence.replace(' ' + cur_pt, '')
    # print(sentence)
    # print(PlaceName)
    return PlaceName, sentence


# [X] Identify Date, Time, Quantity, Percent
# input string sentence:
# 'What buildings are within 1 minute, 2 minutes and 3 minutes of driving time from 3 fire stations that are
# within 60 meters of rivers and located at areas that has slope larger than 10 percent for each PlaceName1 in
# PlaceName2 between 1990 and 2000'
# output tuple:
# ({'Time': [1 minute, 2 minutes, 3 minutes], 'Quantity': [60 meters],
# 'Percent': [larger than 10 percent], 'Date': [between 1990 and 2000]},
# 'What buildings are within ETime0, ETime1, and ETime2 of driving time from 3 fire stations that are within
# EQuantity0 of rivers and located at areas that has slope EPercent1 for each PlaceName0 in PlaceName1 EDate0')
def entity_ner(sentence):
    enti_dict = {}
    Date = []
    Time = []
    Quantity = []
    Percent = []

    cur_sen = ''
    if 'each' in sentence:  # {'Quantity': [each 50 square km]} -> {'Quantity': [50 square km]}
        cur_sen = sentence.replace(' each', '')
    else:
        cur_sen = sentence

    cur_doc = nlp(cur_sen)
    entities = [(i.text, i.label_) for i in cur_doc.ents]
    # e.g., tuple entities = [(1 minute, 'TIME'), (between 1990 and 2000, 'DATE')]

    # print(sentence)
    # print(entities)

    D_loc = 0
    T_loc = 0
    Q_loc = 0
    P_loc = 0

    for i in range(0, len(entities)):
        if entities[i][1] == 'TIME':
            Time.append(entities[i][0])
            sentence = sentence.replace(entities[i][0], 'ETime' + str(T_loc))
            T_loc += 1
        elif entities[i][1] == 'QUANTITY':
            Quantity.append(entities[i][0])
            sentence = sentence.replace(entities[i][0], 'EQuantity' + str(Q_loc))
            Q_loc += 1
        elif entities[i][1] == 'PERCENT':
            Percent.append(entities[i][0])
            sentence = sentence.replace(entities[i][0], 'EPercent' + str(P_loc))
            P_loc += 1
        elif entities[i][1] == 'DATE' and not entities[i][0] == 'annual' and not entities[i][0] == 'monthly' \
                and not entities[i][0].startswith('PlaceName'):
            Date.append(entities[i][0])
            sentence = sentence.replace(entities[i][0], 'EDate' + str(D_loc))
            D_loc += 1

    cur_w = sentence.strip().split(' ')
    cur_quan = ''
    for w in cur_w:
        if w.startswith('meter') or w.startswith('millimeter'):
            cur_quan = cur_w[cur_w.index(w) - 1] + ' ' + w
            Quantity.append(cur_quan)
            sentence = sentence.replace(cur_quan, 'EQuantity' + str(Q_loc))
            Q_loc += 1
        elif w.isnumeric() and cur_w.index(w) < len(cur_w) - 3 and cur_w[cur_w.index(w) + 1] == 'per' and cur_w[
            cur_w.index(w) + 2] == 'square' and cur_w[cur_w.index(w) + 3].startswith(
            'kilometer'):  # 300 per square kilometer
            cur_quan = w + ' per square ' + cur_w[cur_w.index(w) + 3]
            Quantity.append(cur_quan)
            sentence = sentence.replace(cur_quan, 'EQuantity' + str(Q_loc))
            Q_loc += 1
        elif w == 'per' and cur_w.index(w) < len(cur_w) - 3 and cur_w[cur_w.index(w) - 1].isnumeric() and cur_w[
            cur_w.index(w) + 1].isnumeric():  # 500 per 1000000 people
            cur_quan = ' '.join(cur_w[cur_w.index(w) - 1: cur_w.index(w) + 3])
            Quantity.append(cur_quan)
            sentence = sentence.replace(cur_quan, 'EQuantity' + str(Q_loc))
            Q_loc += 1
        elif w.isnumeric() and cur_w[int(cur_w.index(w) - 1)] == 'over' and cur_w[
            int(cur_w.index(w) - 2)] in humanWords:
            Date.append('over ' + w)
            sentence = sentence.replace('over ' + w, 'EDate' + str(D_loc))
            D_loc += 1
        elif w.isnumeric() and cur_w[int(cur_w.index(w) - 1)] == 'than' and cur_w[
            int(cur_w.index(w) - 3)] in humanWords:
            cur_date = ' '.join(cur_w[cur_w.index(w) - 2: cur_w.index(w) + 1])
            Date.append(cur_date)
            sentence = sentence.replace(cur_date, 'EDate' + str(D_loc))
            D_loc += 1

    cur_words = sentence.strip().split(' ')
    if not len(Time) == 0:
        enti_dict['Time'] = Time
    if not len(Quantity) == 0:
        for w in cur_words:
            if w.startswith('EQuantity'):
                i = cur_words.index(w)
                if cur_words[i - 1] == 'by' and cur_words[
                    i - 2].isnumeric():  # 2 by Quantity0(2 km) grid cell -> Quantity0 grid cell
                    Quantity[int(w[9])] = ' '.join(cur_words[i - 2:i]) + ' ' + Quantity[int(w[9])]
                    sentence = sentence.replace(' '.join(cur_words[i - 2:i]) + ' ' + w, w)
                    enti_dict['Quantity'] = Quantity
                elif cur_words[i - 1] == 'from':  # from Quantity0(60 to 600 meters) -> Quantity0
                    Quantity[int(w[9])] = 'from ' + Quantity[int(w[9])]
                    sentence = sentence.replace('from ' + w, w)
                    enti_dict['Quantity'] = Quantity
                elif cur_words[i - 1] == 'to' and cur_words[i - 2].isnumeric() and cur_words[
                    i - 3] == 'from':  # from 300 to Quantity0(900 meters) -> Quantity0
                    Quantity[int(w[9])] = ' '.join(cur_words[i - 3:i]) + ' ' + Quantity[int(w[9])]
                    sentence = sentence.replace(' '.join(cur_words[i - 3:i]) + ' ' + w, w)
                    enti_dict['Quantity'] = Quantity
                elif cur_words[i - 1] == 'and' and cur_words[i - 2].isnumeric() and cur_words[
                    i - 3] == 'between':  # between 700 and Quantity0(2000 meters) -> Quantity0
                    Quantity[int(w[9])] = ' '.join(cur_words[i - 3:i]) + ' ' + Quantity[int(w[9])]
                    sentence = sentence.replace(' '.join(cur_words[i - 3:i]) + ' ' + w, w)
                    enti_dict['Quantity'] = Quantity
                else:
                    enti_dict['Quantity'] = Quantity
    if not len(Percent) == 0:
        enti_dict['Percent'] = Percent
    if not len(Date) == 0:
        for w in cur_words:
            if w.startswith('EDate'):
                i = cur_words.index(w)
                if cur_words[i - 2].isnumeric() and cur_words[i - 1] == 'to':  # from 2000 to Date0 -> Date0
                    Date[int(w[5])] = ' '.join(cur_words[i - 3:i]) + ' ' + Date[int(w[5])]
                    sentence = sentence.replace(' '.join(cur_words[i - 3:i]) + ' ' + w, w)
                    enti_dict['Date'] = Date
                elif i + 2 < len(cur_words) and cur_words[i + 2].isnumeric() and cur_words[
                    i + 1] == 'to':  # from Date0 to 1994
                    Date[int(w[5])] = cur_words[i - 1] + ' ' + Date[int(w[5])] + ' ' + ' '.join(cur_words[i + 1:i + 3])
                    sentence = sentence.replace(cur_words[i - 1] + ' ' + w + ' ' + ' '.join(cur_words[i + 1:i + 3]), w)
                    enti_dict['Date'] = Date
                elif cur_words[i - 1] == 'from' and i + 1 == len(cur_words):  # from Date0 (1997 to 2004)
                    Date[int(w[5])] = 'from ' + Date[int(w[5])]
                    sentence = sentence.replace('from ' + w, w)
                    enti_dict['Date'] = Date
                elif cur_words[i - 1] == 'from' and i + 1 < len(cur_words) and not cur_words[
                                                                                       i + 1] == 'to':  # from Date0 (1997 to 2004) in Utrecht
                    Date[int(w[5])] = 'from ' + Date[int(w[5])]
                    sentence = sentence.replace('from ' + w, w)
                    enti_dict['Date'] = Date
                elif cur_words[i - 1] == 'from' and cur_words[i + 1] == 'to' and cur_words[i + 2].startswith(
                        'Date') and i + 2 < len(cur_words):  # from date0 to date1 -> date0
                    Date[int(w[5])] = 'from ' + Date[int(w[5])] + ' to ' + Date[int(cur_words[i + 2][5])]
                    Date.remove(Date[int(cur_words[i + 2][5])])
                    sentence = sentence.replace('from ' + w + ' to ' + cur_words[i + 2], w)
                    enti_dict['Date'] = Date
                elif cur_words[i - 1] == 'over':  # over 65 years
                    Date[int(w[5])] = 'over ' + Date[int(w[5])]
                    sentence = sentence.replace('over ' + w, w)
                    enti_dict['Date'] = Date
                else:
                    enti_dict['Date'] = Date

    # print(enti_dict)
    # print(sentence)

    return enti_dict, sentence


# Read Core concepts.txt into a dictionary.
def load_ccdict(filePath):
    coreCon = {}
    text = []
    tag = []
    meaLevel = []  # measurement level
    with open(filePath, encoding="utf-8") as coreConcepts:
        for line in coreConcepts:
            cur = line.strip().split('\t')
            text.append(cur[0].lower())
            tag.append(cur[1].lower())
            if len(cur) == 3:
                meaLevel.append(cur[2].lower())
            else:
                meaLevel.append('NULL')
    coreCon['text'] = text
    coreCon['tag'] = tag
    coreCon['measureLevel'] = meaLevel

    return coreCon


# [X] Clean text that would influence noun chunks recognition
# def pre_cc_ner(sentence):
#     cur_s = nt.sent_tokenize(sentence)
#     tokenized_sen = [nt.word_tokenize(cur_sen) for cur_sen in cur_s]  # [['nearest', 'supermarket']]
#     cur_pos = [nltk.pos_tag(c_sen) for c_sen in tokenized_sen][0]  # [('nearest', 'JJS'), ('supermarket', 'NN')]
#     res = [sub[0] for sub in cur_pos if
#            'JJS' in sub[1] or 'JJR' in sub[1] or 'RBS' in sub[1] or 'RBR' in sub[1]]  # ['longest', 'more', 'most']
#
#     if 'most' in res:  # most intense, also remove intense
#         mostIndex = [cur_pos.index(sub) for sub in cur_pos if sub[0] == 'most']
#         nextIndex = mostIndex[0] + 1
#         if cur_pos[nextIndex][1] == 'JJ':
#             res.append(cur_pos[nextIndex][0])
#
#     sen_jjs = [ele for ele in tokenized_sen[0] if ele not in res]
#
#     cur_list = []
#     for cur_word in sen_jjs:
#         if not cur_word.lower() in removeWords and not cur_word.startswith('PlaceName') \
#                 and not cur_word.startswith('Date') and not cur_word.startswith('Quantity') \
#                 and not cur_word.startswith('Time') and not cur_word.startswith('Percent') and not cur_word.isnumeric():
#             cur_list.append(cur_word.lower())
#
#     # Remove many of 'how many', areas of 'what areas'...to avoid (how) many houses in noun_chunks
#     if cur_list[0] == 'area' or cur_list[0] == 'areas' or cur_list[0] == 'many' or cur_list[0] == 'much':
#         cur_list.pop(0)
#
#     cur_que = ' '.join(text for text in cur_list).strip()
#
#     return cur_que


# [X] Clean noun_phrases after noun chunks recognition, remove superlatives and comparatives, placenames, entities...
def noun_phrases_correct(noun_phrases_list):
    global pos
    noun_phrases_CleanList = []
    pos = []

    for cur_noun in noun_phrases_list:
        if cur_noun in cn:
            noun_phrases_CleanList.append(cur_noun)
            # print('noun_phrases_CleanList:', noun_phrases_CleanList)
        else:
            cur_p = nt.sent_tokenize(cur_noun)
            tokenized_sen = [nt.word_tokenize(p) for p in cur_p]  # [['nearest', 'supermarket']]
            cur_pos = [nltk.pos_tag(cur_sen) for cur_sen in tokenized_sen][
                0]  # [('nearest', 'JJS'), ('supermarket', 'NN')]
            for e in cur_pos:
                pos.append(e)
            res = [sub[0] for sub in cur_pos if
                   ('JJS' in sub[1] and not sub[0] == 'west') or 'JJR' in sub[1] or 'RBS' in sub[1] or 'RBR' in sub[
                       1]]  # ['longest', 'more', 'most']

            if 'most' in res or 'more' in res:  # most intense, also remove intense; more than, also remove than
                mostIndex = [cur_pos.index(sub) for sub in cur_pos if sub[0] == 'most' or sub[0] == 'more']
                nextIndex = mostIndex[0] + 1
                if cur_pos[nextIndex][1] == 'JJ' or cur_pos[nextIndex][0] == 'than':
                    res.append(cur_pos[nextIndex][0])

            nounStr_Clean = [ele for ele in tokenized_sen[0] if ele not in res and ele.lower() not in removeWords
                             and not ele.startswith('placename') and not ele.startswith('edate') and not
                             ele.startswith('equantity') and not ele.startswith('etime') and not ele.startswith(
                'epercent')
                             and not ele.startswith(
                'outside') and not ele.isnumeric() and not ele == ',' or ele == '911']
            # print('nounStr_Clean:', nounStr_Clean)

            cur_noun_Clean = ' '.join(text for text in nounStr_Clean).strip()
            # print('cur_noun_Clean:', cur_noun_Clean)

            # [X] remove 'areas' in 'what areas', 'many' in 'how many'...'how many buildings'->'buildings'
            if cur_noun_Clean.startswith('areas'):
                cur_noun_Clean = cur_noun_Clean.replace('areas', '')
            if cur_noun_Clean.startswith('area'):
                cur_noun_Clean = cur_noun_Clean.replace('area', '')
            if cur_noun_Clean.startswith('many'):
                cur_noun_Clean = cur_noun_Clean.replace('many', '')
            if cur_noun_Clean.startswith('much'):
                cur_noun_Clean = cur_noun_Clean.replace('much', '')

            if cur_noun_Clean:
                noun_phrases_CleanList.append(cur_noun_Clean.strip())

    return noun_phrases_CleanList


# [X] Identify Core concepts: field, object, event, network, contentAmount, coverageAmount, conProportion, proportion
# input string sentence: What is number of crime cases for each police district in PlaceName0 in Date0
# output string sentence: what is conamount0 era of event0 for each object0 in placename0 in date0
# output tuple: {'Object': ['police district'], 'Event': ['crime cases'], 'ConAmount': ['number']}
def core_concept_ner(sentence):
    # [X] Noun chunks recognition, and remove Entity tags from detected noun chunks
    # cur_que = pre_cc_ner(sentence)
    cur_sen = sentence
    cur_doc = nlp(cur_sen)
    cur_matches = matcher(cur_doc)
    match_phrases = [cur_doc[start: end].text for mat_id, start, end in cur_matches]
    for cur_ph in match_phrases:
        cur_sen = cur_sen.replace(cur_ph + ' ', '')
    cur_doc2 = nlp(cur_sen)
    noun_list = [noun.text for noun in cur_doc2.noun_chunks]
    if not len(match_phrases) == 0:
        for cur_phr in match_phrases:
            noun_list.append(cur_phr)
    # print(noun_list)

    noun_list_Clean = noun_phrases_correct(noun_list)
    # print(noun_list_Clean)

    # [X] Identify core concepts from noun chunks
    coreConcept_dect = {}
    Field = []
    Object = []
    ObjectQuality = []
    Event = []
    EventQuality = []
    Network = []
    Quality = []
    ConAmount = []
    # ObjConAmount = []
    # EveConAmount = []
    CovAmount = []
    Amount = []
    ConConPro = []
    ConCovPro = []
    CovPro = []
    Proportion = []

    fie_loc = 0
    obj_loc = 0
    objQ_loc = 0
    eve_loc = 0
    eveQ_loc = 0
    net_loc = 0
    qua_loc = 0
    conA_loc = 0
    objConA_loc = 0
    eveConA_loc = 0
    covA_loc = 0
    amou_loc = 0
    conconP_loc = 0
    concovP_loc = 0
    covpro_loc = 0
    pro_loc = 0

    for cur_noun in noun_list_Clean:
        cur_w = cur_noun.split(' ')
        # print('cur_w:', cur_w)
        if cur_noun in coreCon_dict['text'] and not cur_noun == 'population':
            cur_index = coreCon_dict['text'].index(cur_noun)
            if coreCon_dict['tag'][cur_index] == 'field':
                Field.append(cur_noun)
                sentence = sentence.replace(cur_noun,
                                            'field' + str(fie_loc) + ' ' + coreCon_dict['measureLevel'][
                                                cur_index])
                fie_loc += 1
            elif coreCon_dict['tag'][cur_index] == 'object':
                Object.append(cur_noun)
                sentence = sentence.replace(cur_noun, 'object' + str(obj_loc))
                obj_loc += 1
            elif coreCon_dict['tag'][cur_index] == 'object quality':
                ObjectQuality.append(cur_noun)
                sentence = sentence.replace(cur_noun,
                                            'objectquality' + str(objQ_loc) + ' ' +
                                            coreCon_dict['measureLevel'][
                                                cur_index])
                objQ_loc += 1
            elif coreCon_dict['tag'][cur_index] == 'event':
                Event.append(cur_noun)
                sentence = sentence.replace(cur_noun, 'event' + str(eve_loc))
                eve_loc += 1
            elif coreCon_dict['tag'][cur_index] == 'event quality':
                EventQuality.append(cur_noun)
                sentence = sentence.replace(cur_noun,
                                            'eventquality' + str(eveQ_loc) + ' ' + coreCon_dict['measureLevel'][
                                                cur_index])
                eveQ_loc += 1
            elif coreCon_dict['tag'][cur_index] == 'network':
                cur_ns = cur_noun.split(' ')[0]
                cur_i = [x for x, y in enumerate(pos) if y[0] == cur_ns]
                if len(cur_i) >= 1 and (pos[cur_i[0] - 1][1] == 'JJS' or pos[cur_i[0] - 1][1] == 'RBS'):
                    cur_np = pos[cur_i[0] - 1][0] + ' ' + cur_noun
                    Network.append(cur_np)
                    sentence = sentence.lower().replace(cur_np, 'network' + str(net_loc))
                    net_loc += 1
                else:
                    Network.append(cur_noun)
                    sentence = sentence.lower().replace(cur_noun, 'network' + str(net_loc))
                    net_loc += 1
            elif coreCon_dict['tag'][cur_index] == 'quality':
                Quality.append(cur_noun)
                sentence = sentence.replace(cur_noun,
                                            'quality' + str(qua_loc) + ' ' + coreCon_dict['measureLevel'][
                                                cur_index])
                qua_loc += 1
            elif coreCon_dict['tag'][cur_index] == 'covamount':
                CovAmount.append(cur_noun)
                sentence = sentence.replace(cur_noun,
                                            'covamount' + str(covA_loc) + ' ' + coreCon_dict['measureLevel'][
                                                cur_index])
                covA_loc += 1
            elif coreCon_dict['tag'][cur_index] == 'conamount':
                ConAmount.append(cur_noun)
                sentence = sentence.replace(cur_noun,
                                            'conamount' + str(conA_loc) + ' ' + coreCon_dict['measureLevel'][
                                                cur_index])
                conA_loc += 1
            elif coreCon_dict['tag'][cur_index] == 'object conamount':
                ConAmount.append(cur_noun)
                sentence = sentence.replace(cur_noun,
                                            'objconamount' + str(objConA_loc) + ' ' + coreCon_dict['measureLevel'][
                                                cur_index])
                objConA_loc += 1
            elif coreCon_dict['tag'][cur_index] == 'event conamount':
                ConAmount.append(cur_noun)
                sentence = sentence.replace(cur_noun,
                                            'eveconamount' + str(eveConA_loc) + ' ' + coreCon_dict['measureLevel'][
                                                cur_index])
                eveConA_loc += 1
            elif coreCon_dict['tag'][cur_index] == 'objconobjconpro':
                ConConPro.append(cur_noun)
                sentence = sentence.replace(cur_noun,
                                            'objconobjconpro' + str(conconP_loc) + ' ' + coreCon_dict['measureLevel'][
                                                cur_index])
                conconP_loc += 1
            elif coreCon_dict['tag'][cur_index] == 'eveconobjconpro':
                ConConPro.append(cur_noun)
                sentence = sentence.replace(cur_noun,
                                            'eveconobjconpro' + str(conconP_loc) + ' ' + coreCon_dict['measureLevel'][
                                                cur_index])
                conconP_loc += 1
            elif coreCon_dict['tag'][cur_index] == 'conconpro':
                ConConPro.append(cur_noun)
                sentence = sentence.replace(cur_noun,
                                            'conconpro' + str(conconP_loc) + ' ' + coreCon_dict['measureLevel'][
                                                cur_index])
                conconP_loc += 1
            elif coreCon_dict['tag'][cur_index] == 'objconobjcovpro':
                ConCovPro.append(cur_noun)
                sentence = sentence.replace(cur_noun,
                                            'objconobjcovpro' + str(concovP_loc) + ' ' + coreCon_dict['measureLevel'][
                                                cur_index])
                concovP_loc += 1
            elif coreCon_dict['tag'][cur_index] == 'concovpro':
                ConCovPro.append(cur_noun)
                sentence = sentence.replace(cur_noun,
                                            'concovpro' + str(concovP_loc) + ' ' + coreCon_dict['measureLevel'][
                                                cur_index])
                concovP_loc += 1
            elif coreCon_dict['tag'][cur_index] == 'covpro':
                CovPro.append(cur_noun)
                sentence = sentence.replace(cur_noun,
                                            'covpro' + str(covpro_loc) + ' ' + coreCon_dict['measureLevel'][
                                                cur_index])
                covpro_loc += 1
            elif coreCon_dict['tag'][cur_index] == 'proportion':
                Proportion.append(cur_noun)
                sentence = sentence.replace(cur_noun,
                                            'proportion' + str(pro_loc) + ' ' + coreCon_dict['measureLevel'][
                                                cur_index])
                pro_loc += 1
        elif cur_w[0] == 'average' or cur_w[0] == 'median' or cur_w[0] == 'total':  # average Euclidean distance
            Amount.append(cur_w[0])
            sentence = sentence.replace(cur_w[0], 'amount' + str(amou_loc))
            amou_loc += 1
            cur_r = ' '.join(cur_w[1:])  # 'Euclidean' 'distance' -> 'Euclidean distance'
            if cur_r in coreCon_dict['text']:
                cur_in = coreCon_dict['text'].index(cur_r)
                if coreCon_dict['tag'][cur_in] == 'field':
                    Field.append(cur_r)
                    sentence = sentence.replace(cur_r,
                                                'field' + str(fie_loc) + ' ' + coreCon_dict['measureLevel'][cur_in])
                    fie_loc += 1
                elif coreCon_dict['tag'][cur_in] == 'object':
                    Object.append(cur_r)
                    sentence = sentence.replace(cur_r, 'object' + str(obj_loc))
                    obj_loc += 1
                elif coreCon_dict['tag'][cur_in] == 'object quality':
                    ObjectQuality.append(cur_r)
                    sentence = sentence.replace(cur_r,
                                                'objectquality' + str(objQ_loc) + ' ' + coreCon_dict['measureLevel'][
                                                    cur_in])
                    objQ_loc += 1
                elif coreCon_dict['tag'][cur_in] == 'event':
                    Event.append(cur_r)
                    sentence = sentence.replace(cur_r, 'event' + str(eve_loc))
                    eve_loc += 1
                elif coreCon_dict['tag'][cur_in] == 'event quality':
                    EventQuality.append(cur_r)
                    sentence = sentence.replace(cur_r,
                                                'eventquality' + str(eveQ_loc) + ' ' + coreCon_dict['measureLevel'][
                                                    cur_in])
                    eveQ_loc += 1
                elif coreCon_dict['tag'][cur_in] == 'network':
                    Network.append(cur_r)
                    sentence = sentence.lower().replace(cur_r, 'network' + str(net_loc))
                    net_loc += 1
                elif coreCon_dict['tag'][cur_in] == 'quality':
                    Quality.append(cur_r)
                    sentence = sentence.replace(cur_r,
                                                'quality' + str(qua_loc) + ' ' + coreCon_dict['measureLevel'][cur_in])
                    qua_loc += 1
                elif coreCon_dict['tag'][cur_in] == 'covamount':
                    CovAmount.append(cur_r)
                    sentence = sentence.replace(cur_r, 'covamount' + str(covA_loc) + ' ' + coreCon_dict['measureLevel'][
                        cur_in])
                    covA_loc += 1
                elif coreCon_dict['tag'][cur_in] == 'conamount':
                    CovAmount.append(cur_r)
                    sentence = sentence.replace(cur_r, 'conamount' + str(conA_loc) + ' ' + coreCon_dict['measureLevel'][
                        cur_in])
                    conA_loc += 1
                elif coreCon_dict['tag'][cur_in] == 'object conamount':
                    ConAmount.append(cur_r)
                    sentence = sentence.replace(cur_r,
                                                'objconamount' + str(objConA_loc) + ' ' + coreCon_dict['measureLevel'][
                                                    cur_in])
                    objConA_loc += 1
        elif cur_w[len(cur_w) - 1] == 'count' or cur_w[len(cur_w) - 1] == 'counts' or cur_w[
            len(cur_w) - 1] == 'totals' or cur_w[len(cur_w) - 1] == 'total':  # house totals -> Object ConAmount
            ConAmount.append(cur_w[len(cur_w) - 1])
            sentence = sentence.replace(cur_w[len(cur_w) - 1], 'conamount' + str(conA_loc) + ' era')
            conA_loc += 1
            cur_r = ' '.join(cur_w[0:len(cur_w) - 1])
            if cur_r in coreCon_dict['text']:
                cur_in = coreCon_dict['text'].index(cur_r)
                if coreCon_dict['tag'][cur_in] == 'object':
                    Object.append(cur_r)
                    sentence = sentence.replace(cur_r, 'object' + str(obj_loc))
                    obj_loc += 1
                elif coreCon_dict['tag'][cur_in] == 'event':
                    Event.append(cur_r)
                    sentence = sentence.replace(cur_r, 'event' + str(eve_loc))
                    eve_loc += 1
        elif cur_w[len(cur_w) - 1] == 'density':  # household density, tree density
            CovPro.append('density')
            sentence = sentence.replace('density', 'covpro' + str(covpro_loc) + ' ira')
            covpro_loc += 1
            cur_r = ' '.join(cur_w[0:len(cur_w) - 1])
            if cur_r in coreCon_dict['text']:
                cur_in = coreCon_dict['text'].index(cur_r)
                if coreCon_dict['tag'][cur_in] == 'object':
                    Object.append(cur_r)
                    sentence = sentence.replace(cur_r, 'object' + str(obj_loc))
                    obj_loc += 1
                elif coreCon_dict['tag'][cur_in] == 'event':
                    Event.append(cur_r)
                    sentence = sentence.replace(cur_r, 'event' + str(eve_loc))
                    eve_loc += 1
        # elif cur_w[len(cur_w) - 2:len(cur_w)] == ['mortality', 'rate']:
        #     ConConPro.append('mortality rate')
        #     sentence = sentence.replace('mortality rate', 'objconobjconpro' + str(conconP_loc) + ' ira')
        #     conconP_loc += 1
        #     cur_r = ' '.join(cur_w[0:len(cur_w) - 2])
        #     if cur_r in coreCon_dict['text']:
        #         cur_in = coreCon_dict['text'].index(cur_r)
        #         if coreCon_dict['tag'][cur_in] == 'object':
        #             Object.append(cur_r)
        #             sentence = sentence.replace(cur_r, 'object' + str(obj_loc))
        #             obj_loc += 1
        #         elif coreCon_dict['tag'][cur_in] == 'event':
        #             Event.append(cur_r)
        #             sentence = sentence.replace(cur_r, 'event' + str(eve_loc))
        #             eve_loc += 1
        # elif cur_w[len(cur_w) - 1] == 'rate':
        #     ConConPro.append('rate')
        #     sentence = sentence.replace('rate', 'conconpro' + str(conconP_loc) + ' ira')
        #     conconP_loc += 1
        #     cur_r = ' '.join(cur_w[0:len(cur_w) - 1])
        #     if cur_r in coreCon_dict['text']:
        #         cur_in = coreCon_dict['text'].index(cur_r)
        #         if coreCon_dict['tag'][cur_in] == 'object':
        #             Object.append(cur_r)
        #             sentence = sentence.replace(cur_r, 'object' + str(obj_loc))
        #             obj_loc += 1
        #         elif coreCon_dict['tag'][cur_in] == 'object quality':
        #             ObjectQuality.append(cur_r)
        #             sentence = sentence.replace(cur_r, 'objectquality' + str(objQ_loc) + ' ' + coreCon_dict['measureLevel'][cur_in])
        #             objQ_loc += 1
        #         elif coreCon_dict['tag'][cur_in] == 'event':
        #             Event.append(cur_r)
        #             sentence = sentence.replace(cur_r, 'event' + str(eve_loc))
        #             eve_loc += 1
        #         elif coreCon_dict['tag'][cur_in] == 'event quality':
        #             EventQuality.append(cur_r)
        #             sentence = sentence.replace(cur_r, 'eventquality' + str(eveQ_loc) + ' ' + coreCon_dict['measureLevel'][cur_in])

    if 'population' in sentence:
        ConAmount.append('population')
        sentence = sentence.replace('population', 'objconamount' + str(objConA_loc) + ' ' + 'era')
        objConA_loc += 1

    # [X] 'local road' is network in 'What is the potential accessibility by local road for each 2 by 2 km grid cell
    # in Finland'; 'roads' is object in 'Which roads are intersected with forest areas in UK'
    for cur_noun in noun_list_Clean:
        if cur_noun in networkSet:
            if 'network' in sentence or 'access' in cur_sen or 'connectivity' in cur_sen:
                Network.append(cur_noun)
                sentence = sentence.lower().replace(cur_noun, 'network' + str(net_loc))
                net_loc += 1
            else:
                Object.append(cur_noun)
                sentence = sentence.replace(cur_noun, 'object' + str(obj_loc))
                obj_loc += 1

    cur_words = sentence.split(' ')
    for w in cur_words:
        ind = cur_words.index(w)
        if w.startswith('object') and not w.startswith('objectquality') and 'network0' not in cur_words and (
                cur_words[ind - 1].isnumeric() or cur_words[ind - 1] in amsign):
            ConAmount.append('object' + w[-1] + ' conAmount')
            sentence = sentence.replace(w, 'objconamount' + str(len(ConAmount) - 1) + ' era')
        elif w.startswith('event') and 'network0' not in cur_words and (
                cur_words[ind - 1].isnumeric() or cur_words[ind - 1] in amsign):
            ConAmount.append('event' + w[-1] + ' conAmount')
            sentence = sentence.replace(w, 'eveconamount' + str(len(ConAmount) - 1) + ' era')

    if not Field == []:
        coreConcept_dect['Field'] = Field
    if not Object == []:
        coreConcept_dect['Object'] = Object
    if not ObjectQuality == []:
        coreConcept_dect['ObjectQ'] = ObjectQuality
    if not Event == []:
        coreConcept_dect['Event'] = Event
    if not EventQuality == []:
        coreConcept_dect['EventQ'] = EventQuality
    if not Network == []:
        coreConcept_dect['Network'] = Network
    if not Quality == []:
        coreConcept_dect['Quality'] = Quality
    if not ConAmount == []:
        coreConcept_dect['ConAmount'] = ConAmount
    # if not len(ObjConAmount) == 0:
    #     coreConcept_dect['ObjConAmount'] = ObjConAmount
    # if not len(EveConAmount) == 0:
    #     coreConcept_dect['EveConAmount'] = EveConAmount
    if not CovAmount == []:
        coreConcept_dect['CovAmount'] = CovAmount
    if not Amount == []:
        coreConcept_dect['Amount'] = Amount
    if not ConConPro == []:
        coreConcept_dect['ConConPro'] = ConConPro
    if not ConCovPro == []:
        coreConcept_dect['ConCovPro'] = ConCovPro
    if not CovPro == []:
        coreConcept_dect['CovPro'] = CovPro
    if not Proportion == []:
        coreConcept_dect['Proportion'] = Proportion

    # print(coreConcept_dect)

    return coreConcept_dect, sentence.lower()


# [X] Extract parser rules and text from parserTreeString
def get_text(cur_treeStr):
    nodetextDic = {}
    root = BracketMatch(cur_treeStr)
    cur_match = root
    for i in range(len(cur_treeStr)):
        if '(' == cur_treeStr[i]:
            new_match = BracketMatch(cur_treeStr, cur_match, i)
            cur_match.nested_matches.append(new_match)
            cur_match = new_match
        elif ')' == cur_treeStr[i]:
            cur_match.end = i
            cur_match = cur_match.parent
        else:
            continue
    # Here we built the set of matches, now we must print them
    nodes_list = root.nested_matches
    tag = []
    # So we conduct a BFS to visit and print each match...
    while nodes_list != []:
        node = nodes_list.pop(0)
        nodes_list.extend(node.nested_matches)
        nodeStr = str(node).strip()
        nodetextDic.setdefault('tag', []).append(nodeStr.split()[0])
        nodetextDic.setdefault('text', []).append(' '.join(nodeStr.split()[1:][0:len(nodeStr.split()[1:])]))

    return nodetextDic


# [X] Generate parser tree of question by the GeoAnQu grammar and extract core concept transformations
def geo_parser(sentence):
    treeStr = ''
    ques_incorrect = ''
    input = InputStream(sentence)  # [X]sentence =  'What areas are with slope larger than 10 in Spain'
    lexer = GeoAnQuLexer(input)  # get lexer rule
    # lexer.removeErrorListeners()
    # lexer.addErrorListener(MyErrorListener())
    stream = CommonTokenStream(lexer)  # token stream to tokens
    # stream.fill()  # Get all tokens from lexer until EOF
    parser = GeoAnQuParser(stream)
    # parser.addErrorListener(MyErrorListener())
    try:
        tree = parser.start()  # [X] get parsed tree of the sentence
        treeStr = Trees.toStringTree(tree, None, parser)  # Print out a whole tree in LISP form
    except:
        ques_incorrect = sentence

    m_count = treeStr.count('measure')
    con_count = treeStr.count('condition')
    sub_count = treeStr.count('subcon')
    sup_count = treeStr.count('support')
    ext_count = treeStr.count('extent')
    tem_count = treeStr.count('temEx')

    coreConTrans = {}

    quesTextDic = get_text(treeStr)
    coreConTrans['sequence'] = [ele for ele in quesTextDic['tag'] if ele in que_stru]

    if m_count:
        meaTrans = []
        for cur_i in range(0, m_count):
            cur_mea = []
            m_treeStr = Trees.toStringTree(tree.measure()[cur_i], None, parser)
            mTreeDic = get_text(m_treeStr)
            if 'location' in mTreeDic['tag'] and 'allocation' not in mTreeDic['tag']:
                cur_mea.append('location')
            if 'allocation' in mTreeDic['tag']:
                cur_mea.append('allocation')
            if 'conAm' in mTreeDic['tag']:
                cur_mea.append('conAmount')
            if 'coreC' in mTreeDic['tag']:
                coreC_in = [i for i, x in enumerate(mTreeDic['tag']) if x == "coreC"]
                for t in coreC_in:
                    cur_mea.append(mTreeDic['text'][t])  # e.g.['location', 'object 0']'where are the five star hotels'
            if 'grid' in mTreeDic['tag']:
                grid_in = mTreeDic['tag'].index('grid')
                if mTreeDic['tag'][grid_in - 1] == 'coreC':
                    cc_in = cur_mea.index(mTreeDic['text'][grid_in - 1])
                    cur_mea.insert(cc_in + 1, 'grid')
            if 'extremaR' in mTreeDic['tag']:
                ext_in = mTreeDic['tag'].index('extremaR')
                if mTreeDic['tag'][ext_in + 1] == 'coreC':
                    sel_in = cur_mea.index(mTreeDic['text'][ext_in + 1])
                    cur_mea.insert(sel_in, mTreeDic['text'][ext_in + 1] + ' sel')
            if 'extreDist' in mTreeDic['tag']:
                extDist_in = mTreeDic['tag'].index('extreDist')
                if mTreeDic['tag'][extDist_in + 1] == 'coreC':
                    sel_in = cur_mea.index(mTreeDic['text'][extDist_in + 1])
                    cur_mea.insert(sel_in, mTreeDic['text'][extDist_in + 1] + ' sel')
            meaTrans.append(cur_mea)
        coreConTrans['measure'] = meaTrans
    if con_count:
        conTrans = []
        for cur_i in range(0, con_count):
            cur_con = []
            con_treeStr = Trees.toStringTree(tree.condition()[cur_i], None, parser)
            conTextDic = get_text(con_treeStr)
            if 'boolField' in conTextDic['tag']:
                cur_con.append('boolField')
            if 'coreC' in conTextDic['tag']:
                coreC_in = [i for i, x in enumerate(conTextDic['tag']) if x == "coreC"]
                for t in coreC_in:
                    cur_con.append(conTextDic['text'][t])
            if 'grid' in conTextDic['tag']:
                grid_in = conTextDic['tag'].index('grid')
                if conTextDic['tag'][grid_in - 1] == 'coreC':
                    cc_in = cur_con.index(conTextDic['text'][grid_in - 1])
                    cur_con.insert(cc_in + 1, 'grid')
                else:
                    cur_con.append('grid')
            if 'extremaR' in conTextDic['tag']:
                ext_in = conTextDic['tag'].index('extremaR')
                if ext_in+1 < len(conTextDic['tag']) and conTextDic['tag'][ext_in + 1] == 'coreC':
                    sel_in = cur_con.index(conTextDic['text'][ext_in + 1])
                    cur_con.insert(sel_in, conTextDic['text'][ext_in + 1] + ' sel')
                elif ext_in+1 < len(conTextDic['tag']) and conTextDic['tag'][ext_in + 1] == 'boolField':
                    sel_in = cur_con.index('boolField')
                    cur_con.insert(sel_in, 'boolField sel')
                elif len(conTextDic['tag']) == 1:
                    cur_con.index('extremaR')
            if 'extreDist' in conTextDic['tag']:
                extDist_in = conTextDic['tag'].index('extreDist')
                if conTextDic['tag'][extDist_in + 1] == 'coreC':
                    sel_in = cur_con.index(conTextDic['text'][extDist_in + 1])
                    cur_con.insert(sel_in, conTextDic['text'][extDist_in + 1] + ' sel')
                elif conTextDic['tag'][extDist_in + 1] == 'boolField':
                    sel_in = cur_con.index('boolField')
                    cur_con.insert(sel_in, 'boolField sel')
            if 'predR' in conTextDic['tag']:
                predR_in = conTextDic['tag'].index('predR')
                if conTextDic['tag'][predR_in - 1] == 'coreC':
                    sel_in = cur_con.index(conTextDic['text'][predR_in - 1])
                    cur_con.insert(sel_in, conTextDic['text'][predR_in - 1] + ' sel')
                elif conTextDic['tag'][predR_in + 1] == 'coreC':
                    sel_in = cur_con.index(conTextDic['text'][predR_in + 1])
                    cur_con.insert(sel_in, conTextDic['text'][predR_in + 1] + ' sel')
                elif conTextDic['tag'][predR_in + 1] == 'boolField':
                    sel_in = cur_con.index('boolField')
                    cur_con.insert(sel_in, 'boolField sel')
            conTrans.append(
                cur_con)  # ['boolField', 'network 0', 'grid', 'object 0'] 'within equantity0 network0 from grid to object0'
        coreConTrans['condition'] = conTrans
    if sub_count:
        cur_subcon = []
        subcon_treeStr = Trees.toStringTree(tree.subcon(), None, parser)
        subconTextDic = get_text(subcon_treeStr)
        if 'coreC' in subconTextDic['tag']:
            coreC_in = [i for i, x in enumerate(subconTextDic['tag']) if x == "coreC"]
            for t in coreC_in:
                cur_subcon.append(subconTextDic['text'][t])
        if 'predR' in subconTextDic['tag']:
            predR_in = subconTextDic['tag'].index('predR')
            if subconTextDic['tag'][predR_in - 1] == 'coreC':
                sel_in = cur_subcon.index(subconTextDic['text'][predR_in - 1])
                cur_subcon.insert(sel_in, subconTextDic['text'][predR_in - 1] + ' sel')
        coreConTrans['subcondition'] = cur_subcon
    if sup_count:
        cur_sup = []
        sup_treeStr = Trees.toStringTree(tree.support(), None, parser)
        supTextDic = get_text(sup_treeStr)
        if 'coreC' in supTextDic['tag']:
            cc_in = supTextDic['tag'].index('coreC')
            cur_sup.append(supTextDic['text'][cc_in])
        if 'grid' in supTextDic['tag']:
            cur_sup.append('grid')
        if 'distBand' in supTextDic['tag']:
            cur_sup.append('distBand')
        coreConTrans['support'] = cur_sup
    if ext_count:
        for cur_i in range(0, ext_count):
            cur_ext = []
            ext_treeStr = Trees.toStringTree(tree.extent()[cur_i], None, parser)
            extTextDic = get_text(ext_treeStr)
            coreConTrans.setdefault('extent', []).append(extTextDic['text'][0])
    if tem_count:
        tem_treeStr = Trees.toStringTree(tree.temEx(), None, parser)
        temTextDic = get_text(tem_treeStr)
        coreConTrans['temExtent'] = temTextDic['text'][0]

    if 'support' in coreConTrans.keys() and 'coreC' in supTextDic['tag']:
        objq = ['objectquality of ' + coreConTrans['support'][0]]
        coreConTrans['measure'].insert(0, objq)

    if 'grid' in mTreeDic['tag'] or (sup_count and 'grid' in supTextDic['tag']):
        coreConTrans['measure'].insert(0, ['field'])

    # print(coreConTrans)

    return treeStr, ques_incorrect, coreConTrans


if __name__ == '__main__':

    results = []
    nlp_en = CustomEnglish()  # [X] Load English stopwords
    nlp = en_core_web_sm.load()  # load en_core_web_sm of English for NER, noun chunks
    matcher = PhraseMatcher(nlp.vocab)  # add noun phrases when doing noun_chunks
    patterns = [nlp('bus stops'), nlp('driving time'), nlp('grid cell'), nlp('grid cells'), nlp('off street paths'),
                nlp('degree of clustering'), nlp('degree of dispersion'), nlp('fire call'), nlp('fire calls'),
                nlp('wetlands'), nlp('house totals'), nlp('fire hydrant'), nlp('fire scene'), nlp('fire scenes'),
                nlp('owner occupied houses'), nlp('temperature in celsius'), nlp('police beat'), nlp('police beats'),
                nlp('tornado touchdowns'), nlp('nurse practitioner services'), nlp('priority rankings'),
                nlp('plumbing'), nlp('political leaning'), nlp('predicted probability surface'), nlp('fire accidents'),
                nlp('for sale'), nlp('open at')]
    # phrases missed by noun_chunks, add manually
    # [nlp('911 calls'), nlp('precinct'), nlp('forest areas'), nlp('flower stores'), nlp('alarm territory'), nlp('airport')]
    matcher.add("PHRASES", patterns)

    predictorELMo = Predictor.from_path(
        "https://storage.googleapis.com/allennlp-public-models/ner-model-2020.02.10.tar.gz")  # Allennlp Elmo-based NER

    # [X] Read question file
    questionFilepath = 'corpus.txt'
    errorQuestionFilePath = 'error.txt'
    error_ques = open(errorQuestionFilePath, 'w+')

    # afterplace = 'afterplace.txt'
    # ap = open(afterplace, 'w+')
    # afterentity = 'afterentity.txt'
    # ae = open(afterentity, 'w+')
    # afterconcept = 'afterconcept.txt'
    # ac = open(afterconcept, 'w+')
    # afterparser = 'afterparser.txt'
    # apar = open(afterparser, 'w+')

    # [X] Read place type set
    ptypePath = 'Dictionary/place_type.txt'
    pt_set = set(line.strip() for line in open(ptypePath, encoding="utf-8"))

    # [X] Read core concept dictionary
    corePath = 'Dictionary/coreConceptsML.txt'
    coreCon_dict = load_ccdict(corePath)
    networkPath = 'Dictionary/network.txt'
    networkSet = set(l.strip() for l in open(networkPath, encoding="utf-8"))

    pos = []
    amsign = {'have', 'has', 'had', 'no'}
    humanWords = {'people', 'population', 'children'}
    cn = {'least cost route', 'least cost path', 'least costly route', 'least costly path', 'driving time',
          'travel time', 'forest areas', 'for sale', 'open at'}
    removeWords = {'what', 'where', 'which', 'how', 'for', 'each', 'when', 'who', 'why', 'new', 'no', 'similar',
                   'nearest', 'most', 'to', 'at'}

    que_stru = ['measure', 'condition', 'condition', 'subcon', 'support']

    with open(questionFilepath, encoding="utf-8") as questions:
        for question in questions:
            result = {}
            sym = '" ? \n'
            result['Question'] = question.strip(sym)
            # print('---------Question---------')
            # print(result['Question'])

            # [X] Tokenization
            doc = nlp_en(result['Question'])

            # [X] Cleaning text: remove stopwords and save the tokens in a list
            # text = ' '.join([word for word in text if word not in string.punctuation])
            token_list = []
            for word in doc:
                if not word.is_stop and not word.is_punct or word.text == ',':
                    token_list.append(word)
            sen = ' '.join(word.text for word in token_list).strip()  # Question in string without stopwords
            sen_Clean = word2num(sen)  # Convert numeric words into digit numbers
            result['Cleaned_Question'] = sen_Clean

            # XIdentify place names
            re_Place = place_ner(sen_Clean)
            result['PlaceName'] = re_Place[0]  # re_Place[0]: list - PlaceName

            # [X] Identify Date, Time, Quantity, Cardinal, Percent
            re_Entities = entity_ner(re_Place[1])  # parsed_Place[1]: sentence
            result.update(re_Entities[0])  # parsed_Entities[0]: dictionary - Time, Quantity, Percent, Date

            # [X] Identify Core Concept
            re_CoreCon = core_concept_ner(re_Entities[1].lower())  # parsed_Entities[1]: sentence
            result.update(re_CoreCon[0])  # re_CoreCon[0]: dictionary - Core Concepts
            result['NER_Question'] = re_CoreCon[1]  # re_CoreCon[1] : sentence with core concepts holders

            # [X] Generate parser tree & Extract core concept transformation
            parserTree = geo_parser(re_CoreCon[1])
            if parserTree[0]:
                result['TreeStr'] = parserTree[0]
            if parserTree[1]:
                error_ques.write(parserTree[1] + '\n')  # questions can not be parsed in the grammar
            if parserTree[2]:
                result['CoreCTrans'] = parserTree[2]

            results.append(result)

    with open('Results.json', 'w') as outputfile:
        json.dump(results, outputfile)

            # ap.write(re_Place[1] + '\n')
            # ae.write(re_Entities[1] + '\n')
            # ac.write(re_CoreCon[1] + '\n')
            # if parserTree[0]:
            #     apar.write(parserTree[0] + '\n')
            # if parserTree[1]:
            #     apar.write(parserTree[1] + '\n')

    error_ques.close()
    # ap.close()
    # ae.close()
    # ac.close()
    # apar.close()
